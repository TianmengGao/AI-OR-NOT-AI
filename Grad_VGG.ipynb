{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgXsQsEMt2GPFaYub7sh1Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V449gNDIiz1V","executionInfo":{"status":"ok","timestamp":1756547559409,"user_tz":-60,"elapsed":17960,"user":{"displayName":"Tianmeng","userId":"12788940450948297274"}},"outputId":"2881915a-fce9-4a94-e89b-38a157f2452b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import io\n","from tensorflow.keras.models import load_model\n","\n","# Path to the trained model\n","MODEL_PATH = \"/content/drive/MyDrive/best_vgg16_model.h5\"\n","\n","# Load the model without recompiling\n","model = load_model(MODEL_PATH, compile=False)\n","\n","# Print and save the model summary\n","buf = io.StringIO()\n","model.summary(print_fn=lambda x: buf.write(x + \"\\n\"))\n","summary_str = buf.getvalue()\n","\n","print(summary_str)  # print the entire summary\n","with open(\"/content/model_summary.txt\", \"w\") as f:\n","    f.write(summary_str)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"UN7roEHSYRFn","executionInfo":{"status":"ok","timestamp":1756547569852,"user_tz":-60,"elapsed":8987,"user":{"displayName":"Tianmeng","userId":"12788940450948297274"}},"outputId":"a3e22fd3-450e-4886-d4ab-46741b155492"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ vgg16 (Functional)              │ (None, 7, 7, 512)      │    14,714,688 │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d_3      │ (None, 512)            │             0 │\n","│ (GlobalAveragePooling2D)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_6 (Dropout)             │ (None, 512)            │             0 │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_6 (Dense)                 │ (None, 128)            │        65,664 │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_7 (Dropout)             │ (None, 128)            │             0 │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_7 (Dense)                 │ (None, 1)              │           129 │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"," Total params: 14,780,481 (56.38 MB)\n"," Trainable params: 65,793 (257.00 KB)\n"," Non-trainable params: 14,714,688 (56.13 MB)\n","\n","\n"]}]},{"cell_type":"code","source":["#1. Import packages\n","import os, glob\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model, Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","#2. Paths and constants\n","IMG_DIR    = \"/content/drive/MyDrive/feature\"             # directory of images to visualize\n","OUT_DIR    = \"/content/drive/MyDrive/feature_cam_vgg16\"   # output directory\n","MODEL_PATH = \"/content/drive/MyDrive/best_vgg16_model.h5\" # trained VGG16 model\n","IMG_SIZE   = (224, 224)\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","#3. Load trained model\n","base_model = load_model(MODEL_PATH, compile=False)\n","\n","#4. Extract VGG16 backbone and last convolutional layer\n","backbone = base_model.get_layer(\"vgg16\")\n","LAST_CONV_NAME = \"block5_conv3\"\n","last_conv_layer = backbone.get_layer(LAST_CONV_NAME)\n","\n","# Feature sub-model: input -> (last conv features, backbone output)\n","feat_model = Model(inputs=backbone.input,\n","                   outputs=[last_conv_layer.output, backbone.output],\n","                   name=\"vgg16_feat_model\")\n","\n","#5. Rebuild full forward graph: backbone output -> custom head\n","inp = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3), name=\"gradcam_input\")\n","conv_feat_tensor, bb_out = feat_model(inp)\n","\n","# Attach custom classification head\n","x = bb_out\n","attach = False\n","for lyr in base_model.layers:\n","    if attach:\n","        x = lyr(x)\n","    if lyr.name == \"vgg16\":\n","        attach = True\n","\n","func_model = Model(inputs=inp, outputs=x, name=\"vgg16_functional_rebuilt\")\n","grad_model = Model(inputs=func_model.input,\n","                   outputs=[conv_feat_tensor, func_model.output])\n","\n","#6. Grad-CAM core function\n","def make_gradcam_heatmap(img_array, use_logit=False):\n","    with tf.GradientTape() as tape:\n","        conv_out, preds = grad_model(img_array, training=False)\n","        p = preds[:, 0]  # Dense(1, sigmoid) — probability of AI\n","        if use_logit:\n","            eps = 1e-7\n","            p = tf.clip_by_value(p, eps, 1 - eps)\n","            target = tf.math.log(p / (1 - p))\n","        else:\n","            target = p\n","    grads = tape.gradient(target, conv_out)\n","    pooled = tf.reduce_mean(grads, axis=(0, 1, 2))\n","    conv_map = conv_out[0]\n","    heatmap = tf.reduce_sum(conv_map * pooled, axis=-1)\n","    heatmap = tf.nn.relu(heatmap)\n","    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-8)\n","    return heatmap.numpy(), float(p.numpy()[0])\n","\n","def overlay_heatmap(heatmap, bgr_img, alpha=0.45):\n","    h, w = bgr_img.shape[:2]\n","    heatmap = cv2.resize(heatmap, (w, h))\n","    heatmap_uint8 = np.uint8(255 * heatmap)\n","    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n","    return cv2.addWeighted(heatmap_color, alpha, bgr_img, 1 - alpha, 0)\n","\n","#7. Batch processing, saving, and displaying results\n","label_map = {0: \"Human\", 1: \"AI\"}\n","allow_ext = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n","img_paths = sorted(glob.glob(os.path.join(IMG_DIR, \"*.*\")))\n","\n","processed = 0\n","for p in img_paths:\n","    if os.path.splitext(p)[1].lower() not in allow_ext:\n","        continue\n","    bgr = cv2.imread(p)\n","    if bgr is None:\n","        print(f\"[Skip] Cannot read: {p}\")\n","        continue\n","    bgr_resized = cv2.resize(bgr, IMG_SIZE)\n","\n","    ximg = image.img_to_array(cv2.cvtColor(bgr_resized, cv2.COLOR_BGR2RGB))\n","    ximg = np.expand_dims(ximg, axis=0)\n","    ximg = preprocess_input(ximg)  # VGG16 preprocessing\n","\n","    heatmap, prob_ai = make_gradcam_heatmap(ximg, use_logit=False)\n","    pred_cls = 1 if prob_ai >= 0.5 else 0\n","\n","    cam_bgr = overlay_heatmap(heatmap, bgr_resized, alpha=0.45)\n","\n","    # Convert to RGB for display with matplotlib\n","    rgb_orig = cv2.cvtColor(bgr_resized, cv2.COLOR_BGR2RGB)\n","    rgb_cam  = cv2.cvtColor(cam_bgr,     cv2.COLOR_BGR2RGB)\n","\n","    # Plot and save\n","    plt.figure(figsize=(6.5, 3.2), dpi=200)\n","    plt.subplot(1, 2, 1); plt.imshow(rgb_orig); plt.axis(\"off\"); plt.title(\"Original\", fontsize=9)\n","    plt.subplot(1, 2, 2); plt.imshow(rgb_cam);  plt.axis(\"off\")\n","    plt.title(f\"Grad-CAM — Pred: {label_map[pred_cls]} (AI prob={prob_ai:.2f})\", fontsize=9)\n","    plt.tight_layout()\n","\n","    # Save\n","    out_name = os.path.splitext(os.path.basename(p))[0] + \"_cam_vgg16.jpg\"\n","    out_path = os.path.join(OUT_DIR, out_name)\n","    plt.savefig(out_path, bbox_inches=\"tight\")\n","\n","    # Show inline in notebook\n","    plt.show()\n","\n","    processed += 1\n","    print(f\"[Saved and displayed] {out_path}\")\n","\n","print(f\"Done. Exported and displayed {processed} VGG16 Grad-CAM images → {OUT_DIR}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WimdJBLEPLodo-rvm9obPBIgEoidCMlJ"},"id":"_WLyj55pi_nk","executionInfo":{"status":"ok","timestamp":1756547724077,"user_tz":-60,"elapsed":64677,"user":{"displayName":"Tianmeng","userId":"12788940450948297274"}},"outputId":"6fd44d29-2ac2-45bf-9ed1-1fc791cf99ec"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"gBKcVZ5hi_qO"},"execution_count":null,"outputs":[]}]}